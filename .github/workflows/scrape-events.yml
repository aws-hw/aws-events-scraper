name: Scrape AWS Events

on:
  # Run on schedule (daily at 4 AM NZDT / 3 PM UTC)
  schedule:
    - cron: '0 15 * * *'
  
  # Allow manual trigger from GitHub UI
  workflow_dispatch:
  
  # Run on push to main (for testing)
  push:
    branches: [ main ]

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install Chrome
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: stable
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run scraper (with retry)
      run: |
        # Try up to 3 times with 5 minute delay between attempts
        for i in 1 2 3; do
          echo "Attempt $i of 3..."
          if python run_scraper.py; then
            echo "✓ Scraper succeeded on attempt $i"
            exit 0
          else
            echo "✗ Scraper failed on attempt $i"
            if [ $i -lt 3 ]; then
              echo "Waiting 5 minutes before retry..."
              sleep 300
            fi
          fi
        done
        echo "✗ All 3 attempts failed"
        exit 1
    
    - name: Upload to S3
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
      run: |
        # Install AWS CLI
        pip install awscli
        
        # Find the generated Excel file
        EXCEL_FILE=$(ls aws_events_*.xlsx | head -1)
        
        if [ -z "$EXCEL_FILE" ]; then
          echo "Error: No Excel file found"
          exit 1
        fi
        
        echo "Uploading $EXCEL_FILE to S3..."
        
        # Upload to S3 as latest_aws_experience_events_ANZ.xlsx
        aws s3 cp "$EXCEL_FILE" s3://${{ secrets.S3_BUCKET }}/latest_aws_experience_events_ANZ.xlsx
        
        # Also upload with timestamp to archive folder
        aws s3 cp "$EXCEL_FILE" s3://${{ secrets.S3_BUCKET }}/archive/
        
        echo "Upload complete!"
    
    - name: Upload artifact (for debugging)
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraped-events
        path: |
          aws_events_*.xlsx
          debug_screenshot_*.png
          events_output.json
        retention-days: 7
